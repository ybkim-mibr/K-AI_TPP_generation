{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d098a7d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('setting.env')\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d429fad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "target = 'glp1'\n",
    "\n",
    "if target == 'glp1':\n",
    "    DRUGS = ['Byetta', 'Victoza', 'Saxenda', 'Ozempic', 'Wegovy', 'Mounjaro', 'Zepbound', 'Januvia']\n",
    "elif target == 'metformin':\n",
    "    DRUGS = ['Metformin hydrochloride', 'Janumet', 'Synjardy', 'Actoplus Met', 'Glucovance', 'Jentadueto', 'Prandimet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fef19d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get openFDA cfm URLs \n",
    "import requests\n",
    "\n",
    "\n",
    "def get_review_cfm(drugs: list[str]) -> dict:\n",
    "    URL_FORMAT = 'https://api.fda.gov/drug/drugsfda.json?search=openfda.brand_name:\"{drug}\"'\n",
    "    \n",
    "    review_cfm_dict = dict()\n",
    "    for drug in drugs:\n",
    "        try:\n",
    "            response = requests.get(URL_FORMAT.format(drug=drug))\n",
    "            response.raise_for_status()  # handle HTTP errors\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f'{drug} - HTTP Request failed: {e}')\n",
    "            continue\n",
    "\n",
    "        data = response.json().get('results')[0]\n",
    "\n",
    "        for submission in data['submissions']:\n",
    "            for doc in submission.get('application_docs', []):\n",
    "                if doc.get('type', '') != 'Review':\n",
    "                    continue\n",
    "\n",
    "                if doc.get('url', '').endswith('.cfm'):     # TODO: include additional submitted files (PDFs)\n",
    "                    review_cfm_dict.setdefault(drug, []).append(doc)\n",
    "\n",
    "        # TODO: handle marketing_status: Discontinued\n",
    "    return review_cfm_dict\n",
    "\n",
    "\n",
    "review_cfm_dict = get_review_cfm(DRUGS)\n",
    "\n",
    "for k, v in review_cfm_dict.items():\n",
    "    print(k, v[0]['url'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a5c14",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Parse review URLs from HTML\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "\n",
    "def normalize_title(raw_text: str, href: str) -> str:\n",
    "    txt = re.sub(r\"\\s+\", \" \", raw_text).strip()\n",
    "    href_l = (href or \"\").lower()\n",
    "\n",
    "    m = re.match(r\"^part\\s*(\\d+)$\", txt, flags=re.I)\n",
    "    if m:\n",
    "        part = m.group(1)\n",
    "        if \"clinpharm\" in href_l:\n",
    "            return f\"Clinical Pharmacology Biopharmaceutics Review(s) - Part {part}\"\n",
    "        if \"pharmr\" in href_l:\n",
    "            return f\"Pharmacology Review(s) - Part {part}\"\n",
    "        if \"medr\" in href_l or \"clinical\" in href_l:\n",
    "            return f\"Clinical Review(s) - Part {part}\"\n",
    "        return f\"Review - Part {part}\"\n",
    "\n",
    "    return txt\n",
    "\n",
    "\n",
    "def get_review_url(review_cfm_dict: dict) -> dict:\n",
    "    review_url_dict = dict()\n",
    "\n",
    "    for drug in review_cfm_dict.keys():\n",
    "        for doc in review_cfm_dict[drug]:\n",
    "            review_url = doc['url']\n",
    "            base_url = review_url.rsplit('/', 1)[0] + '/'\n",
    "\n",
    "            try:\n",
    "                resp = requests.get(review_url, timeout=30)\n",
    "                resp.raise_for_status()\n",
    "            except requests.HTTPError as e:\n",
    "                if resp.status_code == 503:\n",
    "                    time.sleep(60)\n",
    "                    resp = requests.get(review_url, timeout=30)\n",
    "\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            \n",
    "            toc_links = soup.select('ul li a[href]')\n",
    "            for a in toc_links:\n",
    "                if 'pharmr' in a['href'].lower() or 'medr' in a['href'].lower():\n",
    "                    key = normalize_title(a.get_text(\" \", strip=True), a['href'])\n",
    "                    review_url_dict.setdefault(drug, dict())[key] = urljoin(base_url, a['href'])\n",
    "    \n",
    "    return review_url_dict\n",
    "\n",
    "\n",
    "review_url_dict = get_review_url(review_cfm_dict)\n",
    "\n",
    "for k, v in review_url_dict.items():\n",
    "    print(k, v, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06353eb6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Upload files\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "\n",
    "\n",
    "def load_file_index(file_index_filename: str) -> dict:\n",
    "    if os.path.exists(file_index_filename):\n",
    "        with open(file_index_filename, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                existing = json.load(f)\n",
    "                return existing if isinstance(existing, dict) else dict()\n",
    "            except json.JSONDecodeError:\n",
    "                return dict()\n",
    "    return dict()\n",
    "\n",
    "\n",
    "def save_file_index(file_index: dict, file_index_filename: str) -> None:\n",
    "    with open(file_index_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(file_index, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "def upload_pdf(url: str, filename: str) -> str:\n",
    "    try:\n",
    "        r = requests.get(url, stream=True, timeout=60)\n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(filename, url)\n",
    "        return\n",
    "    \n",
    "    content = io.BytesIO(r.content)\n",
    "    f = client.files.create(file=(filename, content), purpose='assistants')\n",
    "    return f.id\n",
    "\n",
    "\n",
    "def upload_missing_pdfs(review_url_dict: dict, existing_index: dict) -> tuple[list, list]:\n",
    "    file_ids = []\n",
    "    file_meta = []\n",
    "\n",
    "    for drug, docs in review_url_dict.items():\n",
    "        for title, pdf_url in docs.items():\n",
    "            if existing_index.get(drug, {}).get(title):\n",
    "                continue\n",
    "\n",
    "            filename = f'{drug}_{title}.pdf'\n",
    "            fid = upload_pdf(pdf_url, filename)\n",
    "\n",
    "            if fid:\n",
    "                file_ids.append(fid)\n",
    "                file_meta.append((fid, {'drug': drug, 'review': filename}))\n",
    "                existing_index.setdefault(drug, {})[filename] = fid\n",
    "\n",
    "    return file_ids, file_meta\n",
    "\n",
    "\n",
    "file_index_filename = 'file_index.json'\n",
    "existing = load_file_index(file_index_filename)\n",
    "file_ids, file_meta = upload_missing_pdfs(review_url_dict, existing)\n",
    "save_file_index(existing, file_index_filename)\n",
    "\n",
    "print(f'new uploads: {len(file_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44071100",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an expert in drafting Target Product Profiles (TPPs) based strictly on regulatory and drug development source documents.\n",
    "Prepare the TPP solely based on the provided documents.\n",
    "\n",
    "Output Requirements (Strict):\n",
    "- You must write a total of 17 sections, in the exact order specified.\n",
    "- Each section must include all three components below:\n",
    "  1) [Conclusion]\n",
    "    - Content intended for inclusion in the corresponding TPP section\n",
    "  2) [Evidence]\n",
    "    - Up to five (5) bullet points citing document-based evidence\n",
    "    - Page numbers are mandatory\n",
    "    - Include referenced sentences, sections, figures, tables, or table numbers where available\n",
    "- Maintain the same structure and formatting for every section.\n",
    "- Construct sentences primarily using terminology and phrasing found in the source documents.\n",
    "\n",
    "Behavioral Rules (Critical):\n",
    "- All claims must be supported by evidence from the provided documents obtained via file_search.\n",
    "- Do not infer, assume, or generalize beyond what is explicitly stated in the documents.\n",
    "- If no supporting evidence exists, explicitly state “No supporting evidence found in the documents.”\n",
    "- The [Evidence] section must be written only after performing file_search.\n",
    "- If file_search yields no relevant results, state “No supporting evidence found in the documents.”\n",
    "- Before drafting each section, internally construct relevant search queries and perform file_search.\n",
    "- Write from a preclinical-stage perspective prior to IND submission.\n",
    "- Do not make clinical assumptions or claims.\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"Writing Objective: Draft a Target Product Profile(TPP) – Preclinical Stage (Prior to IND Submission)\n",
    "\n",
    "Before writing each section, internally construct appropriate search queries and perform file_search against the provided documents to locate relevant evidence.\n",
    "(Example queries: “indication”, “dose”, “toxicology”, “NOAEL”, “safety pharmacology”, “PK”, “heart rate”, “monkey study”, etc.)\n",
    "\n",
    "=== TPP Section ===\n",
    "1. Indication and Usage\n",
    "2. Dosage and Administration\n",
    "3. Dosage Forms and Strengths\n",
    "4. Contraindications\n",
    "5. Warnings and Precautions\n",
    "6. Adverse Reactions\n",
    "7. Drug Interactions\n",
    "8. Use in Specific Populations\n",
    "9. Drug Abuse and Dependence\n",
    "10. Overdosage\n",
    "11. Description\n",
    "12. Clinical Pharmacology\n",
    "13. Nonclinical Pharmacology\n",
    "14. Clinical Studies\n",
    "15. How Supplied/Storage and Handling\n",
    "16. Patient Counseling Information\n",
    "\n",
    "Required Format(Apply to Every Section)\n",
    "## 1. Indication and Usage\n",
    "[Conclusion] ...\n",
    "[Evidence]\n",
    "- ...\n",
    "- ...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70892420",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def create_client_response_w_rag(vector_store_id: str) -> Any:\n",
    "    resp = client.responses.create(\n",
    "        model=\"gpt-5.1\",\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"input_text\", \"text\": SYSTEM_PROMPT}]},\n",
    "            {\"role\": \"user\", \"content\": [{\"type\": \"input_text\", \"text\": USER_PROMPT}]}\n",
    "        ],\n",
    "        tools=[{\n",
    "            \"type\": \"file_search\",\n",
    "            \"vector_store_ids\": [vector_store_id],\n",
    "        }],\n",
    "        include=[\"file_search_call.results\"],\n",
    "        reasoning={\"effort\": \"high\"}\n",
    "    )\n",
    "    return resp\n",
    "\n",
    "\n",
    "def create_client_response_wo_rag(file_ids: str|list) -> Any:\n",
    "    if isinstance(file_ids, str):\n",
    "        file_ids = [file_ids]\n",
    "\n",
    "    user_content = [{\"type\": \"input_file\", \"file_id\": fid} for fid in file_ids]\n",
    "    user_content.append({\"type\": \"input_text\", \"text\": USER_PROMPT})\n",
    "    \n",
    "    resp = client.responses.create(\n",
    "        model=\"gpt-5.1\",\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"input_text\", \"text\": SYSTEM_PROMPT}]},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ],\n",
    "        reasoning={\"effort\": \"high\"}\n",
    "    )\n",
    "    return resp\n",
    "\n",
    "\n",
    "def create_client_response_hybrid(file_ids: str|list, vector_store_id: str) -> Any:\n",
    "    if isinstance(file_ids, str):\n",
    "        file_ids = [file_ids]\n",
    "\n",
    "    user_content = [{\"type\": \"input_file\", \"file_id\": fid} for fid in file_ids]\n",
    "    user_content.append({\"type\": \"input_text\", \"text\": USER_PROMPT})\n",
    "\n",
    "    resp = client.responses.create(\n",
    "        model=\"gpt-5.1\",\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"input_text\", \"text\": SYSTEM_PROMPT}]},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ],\n",
    "        tools=[{\n",
    "            \"type\": \"file_search\",\n",
    "            \"vector_store_ids\": [vector_store_id],\n",
    "        }],\n",
    "        # include=[\"file_search_call.results\"],\n",
    "        reasoning={\"effort\": \"high\"}\n",
    "    )\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a78e060",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def check_file_status(vs_id: str) -> list:\n",
    "    failed = []\n",
    "    for file in client.vector_stores.files.list(vector_store_id=vs_id).data:\n",
    "        if file.status == 'failed':\n",
    "            failed.append((file.id, file.last_error))\n",
    "    return failed\n",
    "\n",
    "\n",
    "def check_vector_store_status(vs_id: str) -> None:\n",
    "    while True:\n",
    "        vs = client.vector_stores.retrieve(vs_id)\n",
    "        print(vs.id, vs.status, vs.file_counts)\n",
    "\n",
    "        if vs.status == \"completed\" and vs.file_counts.total == vs.file_counts.completed and vs.file_counts.total != 0:\n",
    "            break\n",
    "\n",
    "        for file_id, err in check_file_status(vs_id):\n",
    "            code = getattr(err, \"code\", None)\n",
    "            print(f\"FAILED: {file_id} - {err} (code={code})\")\n",
    "\n",
    "            if code == \"server_error\":\n",
    "                pass\n",
    "            else:\n",
    "                raise RuntimeError(f\"Vector store has failed files. file_id={file_id}, last_error={err}\")\n",
    "\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8692d635",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "process = 'clinical'\n",
    "\n",
    "with open(f'{target}.txt', 'a') as file:\n",
    "    for drug in list(set(existing.keys()) & set(DRUGS))[:3]:\n",
    "        print(f'========== {drug} ==========')\n",
    "        file_ids = []\n",
    "        for file_name, file_id in existing.get(drug).items():\n",
    "            if process == 'preclinical' and file_name.split('_', 1).lower().startswith('clinical'):\n",
    "                continue\n",
    "            file_ids.append(file_id)\n",
    "\n",
    "        if file_ids:\n",
    "            vs = client.vector_stores.create(\n",
    "                name=drug,\n",
    "                file_ids=file_ids\n",
    "            )\n",
    "            check_vector_store_status(vs.id)\n",
    "\n",
    "            file.write(f'===== {drug} =====\\n')\n",
    "            try:\n",
    "                resp = create_client_response_hybrid(file_ids=file_ids, vector_store_id=vs.id)\n",
    "                file.write(resp.output_text)\n",
    "            except Exception as e:\n",
    "                file.write(str(e))  # TODO: handle errors\n",
    "            file.write('\\n\\n')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
